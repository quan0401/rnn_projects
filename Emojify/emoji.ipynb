{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from my_utils import read_glove_vec, softmax, xavier_init, read_csv, print_predictions, label_to_emoji\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Activation\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = read_csv('./data/train_emoji.csv')\n",
    "X_test, Y_test = read_csv('./data/tesss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = len(max(X, key=lambda x: len(x.split()) ).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132,) (132,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emojifier - V1\n",
    "In the first half of this notebook is to replicate the model below. \n",
    "\n",
    "Then we try to train and evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/image_1.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec, word_to_index, index_to_word = read_glove_vec('./data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embedding features: 50\n"
     ]
    }
   ],
   "source": [
    "emb_features = list(word_to_vec.values())[0].shape[0]\n",
    "print('Number of embedding features:',emb_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_embeddings(sentence: str, word_to_vec: dict): \n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        sentence (str): a target sentence\n",
    "        word_to_vec (dict of ndarray): a dictionary with keys are words, each value has shape (emb_features,)\n",
    "        \n",
    "    Returns:\n",
    "        embeddings (ndarray, (n_words, emb_features))\n",
    "    \"\"\"\n",
    "    words = sentence.lower().split()\n",
    "    embeddings = np.array([word_to_vec[w] for w in words])\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test \n",
    "embeddings = sentence_to_embeddings('Hello friend tram', word_to_vec)\n",
    "assert embeddings.shape == (3, emb_features), 'Wrong shape for embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_embeddings(sentence: str, word_to_vec: dict): \n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        sentence (str): a target sentence\n",
    "        word_to_vec (dict of ndarray): a dictionary with keys are words, each value has shape (emb_features,)\n",
    "        \n",
    "    Returns:\n",
    "        avg (ndarray, (1, emb_features))\n",
    "    \"\"\"\n",
    "    embeddings = sentence_to_embeddings(sentence, word_to_vec)\n",
    "    n_words, emb_features = embeddings.shape\n",
    "    \n",
    "    avg = np.zeros((1, emb_features))\n",
    "    # Sum all e\n",
    "    for e in embeddings: \n",
    "        avg = avg + e\n",
    "    # Take average\n",
    "    avg = avg / n_words\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = avg_embeddings('Hello friend tram', word_to_vec)\n",
    "assert avg.shape == (1, emb_features), 'Wrong shape for avg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "W, b = xavier_init(5, emb_features)\n",
    "parameters = {}\n",
    "parameters['W'] = W\n",
    "parameters['b'] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence: str, word_to_vec: dict, parameters: dict): \n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        sentence (str): a target sentence\n",
    "        word_to_vec (dict of ndarray): a dictionary with keys are words, each value has shape (emb_features,)\n",
    "        \n",
    "    Returns:\n",
    "        a (ndarray, (n_classes, m))\n",
    "    \"\"\"\n",
    "    avg = avg_embeddings(sentence, word_to_vec)\n",
    "    W, b = parameters['W'], parameters['b']\n",
    "    z = np.dot(W, avg.T) + b\n",
    "    \n",
    "    a = softmax(z)\n",
    "    return a, avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, avg = predict('Hello friend tram', word_to_vec, parameters)\n",
    "assert a.shape == (b.shape[0], 1), 'Wrong shape for a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y: ndarray): \n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        Y (m,): _description_\n",
    "\n",
    "    Returns:\n",
    "        Y (n_classes, m)\n",
    "    \"\"\"\n",
    "    return np.array(tf.one_hot(Y, depth=len(np.unique(Y)))).T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 18:57:59.216514: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max\n",
      "2023-12-28 18:57:59.216533: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2023-12-28 18:57:59.216539: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2023-12-28 18:57:59.216566: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-12-28 18:57:59.216581: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 410.43365788314725\n",
      "Epoch: 100 --- cost = 18.682235704620712\n",
      "Epoch: 200 --- cost = 10.867104727024016\n",
      "Epoch: 300 --- cost = 0.3004909068288898\n",
      "Epoch: 400 --- cost = 0.21364235173925147\n"
     ]
    }
   ],
   "source": [
    "def train_model(X: ndarray[str], Y: ndarray, n_iters: int, learning_rate: float): \n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        X (ndarray, (m,)): senetences\n",
    "        Y (ndarray, (m,)): chosen emojies indices for every sentences \n",
    "        n_iters (int): number of iterations\n",
    "        learning_rate (float): learning rate\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    n_y = len(np.unique(Y))\n",
    "    np.random.seed(1)\n",
    "    W, b = xavier_init(n_y, emb_features)\n",
    "    parameters['W'] = W\n",
    "    parameters['b'] = b\n",
    "    \n",
    "    Y_hot = convert_to_one_hot(Y)\n",
    "\n",
    "    for t in range(n_iters): \n",
    "        cost = 0\n",
    "        dW = 0\n",
    "        db = 0\n",
    "        \n",
    "        for i in range(m): \n",
    "            a, avg = predict(X[i], word_to_vec, parameters)\n",
    "            y_i = np.expand_dims(Y_hot[:, i], axis=-1) \n",
    "            cost += -np.sum(y_i  * np.log(a)) \n",
    "\n",
    "            # Compute gradients\n",
    "            dz = a - y_i \n",
    "            dW += np.dot(dz, avg)\n",
    "            db += dz\n",
    "\n",
    "            # Update parameters with Stochastic Gradient Descent\n",
    "            parameters['W'] = parameters['W'] - learning_rate * dW\n",
    "            parameters['b'] = parameters['b'] - learning_rate * db\n",
    "            \n",
    "        if t % 100 == 0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "        \n",
    "        \n",
    "    return parameters\n",
    "\n",
    "parameters = train_model(X, Y, n_iters=401, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(X, Y): \n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        X (m,): list of setences\n",
    "        Y (m,): list of true labels\n",
    "\n",
    "    Returns:\n",
    "        accuracy (float): accuracy on given set\n",
    "        ypred (nadarray, (m,)): model predictions\n",
    "    \"\"\"\n",
    "    accurate_count = 0  \n",
    "    m = len(X)\n",
    "    ypred = []\n",
    "    for i in range(m): \n",
    "        a_i, avg = predict(X[i], word_to_vec, parameters)\n",
    "        ypred_i = np.argmax(a_i[:, 0])\n",
    "        ypred.append(ypred_i)\n",
    "        if ypred_i == int(Y[i]): \n",
    "            \n",
    "            accurate_count += 1\n",
    "    accuracy = accurate_count / m\n",
    "    return accuracy, np.array(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, ypred = compute_accuracy(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_my_sentences = np.array([\"i treasure you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"have you eaten yet\"])\n",
    "Y_my_labels = np.array([0, 0, 2, 1, 4, 4])\n",
    "\n",
    "acc, pred = compute_accuracy(X_my_sentences, Y_my_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "i treasure you â£ï¸\n",
      "i love you â£ï¸\n",
      "funny lol ğŸ˜‚\n",
      "lets play with a ball âš¾\n",
      "food is ready ğŸ´\n",
      "have you eaten yet ğŸ´\n"
     ]
    }
   ],
   "source": [
    "print_predictions(X_my_sentences, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "i treasure you â£ï¸\n",
      "i love you â£ï¸\n",
      "funny lol ğŸ˜‚\n",
      "lets play with a ball âš¾\n",
      "food is ready ğŸ´\n",
      "have you eaten yet ğŸ´\n"
     ]
    }
   ],
   "source": [
    "print_predictions(X_my_sentences, Y_my_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras model\n",
    "This is the model that we try to replicate by `tensorflow`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/emojifier-v2.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "def pretrained_embedding_layer(word_to_vec: Dict[str, np.ndarray], word_to_index: Dict[str, int]):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        word_to_vec (Dict[str, np.ndarray(emb_f,)]): map word to its embedding vector\n",
    "        word_to_index (Dict[str, int]): mapping from words to their indices in vocabulary\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_to_index) + 1 # adding 1 to fit Keras embedding (required)\n",
    "    any_word = list(word_to_vec.keys())[0]\n",
    "    emb_f = word_to_vec[any_word].shape[0]\n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_size, emb_f))\n",
    "    \n",
    "    for w, i in word_to_index.items():\n",
    "        emb_matrix[i, :] = word_to_vec[w]\n",
    "\n",
    "    embedding_layer = Embedding(input_dim=vocab_size, \n",
    "                                output_dim=emb_f, \n",
    "                                trainable=False)\n",
    "    \n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    return embedding_layer\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m,)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (â‰ˆ 1 line)\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "        \n",
    "        # Convert the ith training sentence to lower case and split it into words. You should get a list of words.\n",
    "        sentence_words = X[i].lower().split()\n",
    "        \n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "        available_word = word_to_index\n",
    "        for w in sentence_words:\n",
    "            # if w exists in the word_to_index dictionary\n",
    "            if w in available_word:\n",
    "                # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "                X_indices[i, j] = available_word[w]\n",
    "                # Increment j to j + 1\n",
    "                j += 1\n",
    "            \n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Emojify_V2(input_shape, word_to_vec, word_to_index): \n",
    "    \"\"\"\n",
    "    Function creating the Emojify-v2 model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence_indices = tf.keras.Input(input_shape, dtype='int32')\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec, word_to_index)\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    \n",
    "    X = Dropout(0.5)(X)\n",
    "    \n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "\n",
    "    X = Dropout(0.5)(X)\n",
    "    \n",
    "    X = Dense(5)(X)\n",
    "    \n",
    "    X = Activation('softmax')(X)\n",
    "    model = tf.keras.Model(inputs=sentence_indices, outputs=X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 10)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 10, 50)            20000050  \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 10, 128)           91648     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 645       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20223927 (77.15 MB)\n",
      "Trainable params: 223877 (874.52 KB)\n",
      "Non-trainable params: 20000050 (76.29 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Emojify_V2((maxLen,), word_to_vec, word_to_index)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 10)\n",
      "(132, 5)\n"
     ]
    }
   ],
   "source": [
    "X_train_indices = sentences_to_indices(X, word_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(Y).T\n",
    "print(X_train_indices.shape)\n",
    "print(Y_train_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 18:58:02.760239: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 2s 69ms/step - loss: 1.5897 - accuracy: 0.2652\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 1.5967 - accuracy: 0.2955\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 1.5213 - accuracy: 0.3333\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 1.5271 - accuracy: 0.3106\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 1.5057 - accuracy: 0.3258\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1.4334 - accuracy: 0.3864\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1.4340 - accuracy: 0.3788\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 1.3496 - accuracy: 0.4470\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 1.2942 - accuracy: 0.4924\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 1.1816 - accuracy: 0.5303\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1.0355 - accuracy: 0.6439\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.9052 - accuracy: 0.6970\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.8137 - accuracy: 0.6667\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.6958 - accuracy: 0.7500\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.6331 - accuracy: 0.7652\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.7506 - accuracy: 0.7121\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.6573 - accuracy: 0.7727\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.5421 - accuracy: 0.7879\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4922 - accuracy: 0.8333\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4245 - accuracy: 0.8258\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4148 - accuracy: 0.8636\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.4458 - accuracy: 0.8409\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.3386 - accuracy: 0.8864\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.2625 - accuracy: 0.8939\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.2738 - accuracy: 0.8939\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.3489 - accuracy: 0.8788\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.2893 - accuracy: 0.8788\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3086 - accuracy: 0.9015\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3710 - accuracy: 0.8712\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.4394 - accuracy: 0.8258\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.2865 - accuracy: 0.8788\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3992 - accuracy: 0.8636\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4371 - accuracy: 0.8485\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3766 - accuracy: 0.8485\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.2693 - accuracy: 0.9167\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.2640 - accuracy: 0.9242\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.2048 - accuracy: 0.9242\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.1744 - accuracy: 0.9470\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.1539 - accuracy: 0.9470\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.1369 - accuracy: 0.9621\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.1177 - accuracy: 0.9621\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0949 - accuracy: 0.9773\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0852 - accuracy: 0.9773\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.1313 - accuracy: 0.9621\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0609 - accuracy: 0.9924\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.1142 - accuracy: 0.9621\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.1378 - accuracy: 0.9697\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.1548 - accuracy: 0.9545\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0699 - accuracy: 0.9773\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.1096 - accuracy: 0.9545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2968c2980>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 51ms/step - loss: 0.8796 - accuracy: 0.7857\n",
      "\n",
      "Test accuracy =  0.7857142686843872\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "Y_test_oh = convert_to_one_hot(Y_test).T\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected emoji: ğŸ´| I want to eat\t ğŸ´\n",
      "Expected emoji: ğŸ˜| he did not answer\t ğŸ˜\n",
      "Expected emoji: ğŸ˜‚| he got a very nice raise\t ğŸ˜‚\n",
      "Expected emoji: ğŸ˜‚| she got me a nice present\t ğŸ˜‚\n",
      "Expected emoji: ğŸ˜‚| ha ha ha it was so funny\t ğŸ˜‚\n",
      "Expected emoji: ğŸ˜‚| he is a good friend\t ğŸ˜‚\n",
      "Expected emoji: ğŸ˜| I am upset\t ğŸ˜\n",
      "Expected emoji: ğŸ˜‚| We had such a lovely dinner tonight\t ğŸ˜‚\n",
      "Expected emoji: ğŸ´| where is the food\t ğŸ´\n",
      "Expected emoji: ğŸ˜‚| Stop making this joke ha ha ha\t ğŸ˜‚\n",
      "Expected emoji: âš¾| where is the ball\t âš¾\n",
      "Expected emoji: ğŸ˜| work is hard\t ğŸ˜‚\n",
      "Expected emoji: ğŸ˜| This girl is messing with me\t ğŸ˜\n",
      "Expected emoji: ğŸ˜| are you serious ğŸ˜\n",
      "Expected emoji: âš¾| Let us go play baseball\t âš¾\n",
      "Expected emoji: ğŸ˜| This stupid grader is not working \t ğŸ˜\n",
      "Expected emoji: ğŸ˜| work is horrible\t ğŸ˜\n",
      "Expected emoji: ğŸ˜‚| Congratulation for having a baby\t ğŸ˜‚\n",
      "Expected emoji: ğŸ˜| stop pissing me off ğŸ˜\n",
      "Expected emoji: ğŸ´| any suggestions for dinner\t ğŸ´\n",
      "Expected emoji: â£ï¸| I love taking breaks\t ğŸ˜\n",
      "Expected emoji: ğŸ˜‚| you brighten my day\t ğŸ˜‚\n",
      "Expected emoji: ğŸ´| I boiled rice\t ğŸ´\n",
      "Expected emoji: ğŸ˜| she is a bully\t ğŸ˜\n",
      "Expected emoji: ğŸ˜| Why are you feeling bad\t ğŸ˜\n",
      "Expected emoji: ğŸ˜| I am upset\t ğŸ˜\n",
      "Expected emoji: âš¾| give me the ball âš¾\n",
      "Expected emoji: â£ï¸| My grandmother is the love of my life\t â£ï¸\n",
      "Expected emoji: âš¾| enjoy your game âš¾\n",
      "Expected emoji: ğŸ˜‚| valentine day is near\t ğŸ˜‚\n",
      "Expected emoji: â£ï¸| I miss you so much\t â£ï¸\n",
      "Expected emoji: âš¾| throw the ball\t âš¾\n",
      "Expected emoji: ğŸ˜| My life is so boring\t ğŸ˜‚\n",
      "Expected emoji: ğŸ˜‚| she said yes\t ğŸ˜‚\n",
      "Expected emoji: ğŸ˜‚| will you be my valentine\t â£ï¸\n",
      "Expected emoji: âš¾| he can pitch really well\t âš¾\n",
      "Expected emoji: ğŸ˜‚| dance with me\t ğŸ˜‚\n",
      "Expected emoji: ğŸ´| I am hungry ğŸ´\n",
      "Expected emoji: ğŸ´| See you at the restaurant\t ğŸ´\n",
      "Expected emoji: ğŸ˜‚| I like to laugh\t ğŸ˜‚\n",
      "Expected emoji: âš¾| I will  run âš¾\n",
      "Expected emoji: â£ï¸| I like your jacket \t â£ï¸\n",
      "Expected emoji: â£ï¸| i miss her\t â£ï¸\n",
      "Expected emoji: âš¾| what is your favorite baseball game\t âš¾\n",
      "Expected emoji: ğŸ˜‚| Good job\t ğŸ˜‚\n",
      "Expected emoji: â£ï¸| I love you to the stars and back\t âš¾\n",
      "Expected emoji: ğŸ˜‚| What you did was awesome\t ğŸ˜‚\n",
      "Expected emoji: ğŸ˜‚| ha ha ha lol\t ğŸ˜‚\n",
      "Expected emoji: ğŸ˜| I do not want to joke\t ğŸ˜\n",
      "Expected emoji: ğŸ˜| go away\t ğŸ˜\n",
      "Expected emoji: ğŸ˜| yesterday we lost again\t ğŸ˜\n",
      "Expected emoji: â£ï¸| family is all I have\t â£ï¸\n",
      "Expected emoji: ğŸ˜| you are failing this exercise\t ğŸ˜\n",
      "Expected emoji: ğŸ˜‚| Good joke\t ğŸ˜‚\n",
      "Expected emoji: ğŸ˜‚| You deserve this nice prize\t ğŸ˜‚\n",
      "Expected emoji: ğŸ´| I did not have breakfast  ğŸ´\n"
     ]
    }
   ],
   "source": [
    "acc, ypred = compute_accuracy(X_test, Y_test)\n",
    "print_predictions(X_test, ypred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
